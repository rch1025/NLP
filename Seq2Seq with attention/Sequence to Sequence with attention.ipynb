{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### seq2seq with attention code를 하나씩 뜯어보기\n",
    "- 본 코드는 유투버 나동빈 님의 seq2seq 논문 재현 코드입니다.\n",
    "- 오리지널 코드에도 자세하게 달려있으나 제가 하나씩 뜯어보면서 이해한 부분들을 조금 더 상세하게 적어놓았으니 참고하면 좋을 것 같습니다.\n",
    "- original code link: https://github.com/ndb796/Deep-Learning-Paper-Review-and-Practice/blob/master/code_practices/Sequence_to_Sequence_with_Attention_Tutorial.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!python -m spacy download en\n",
    "!python -m spacy download de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "spacy_en = spacy.load('en_core_web_sm') # 영어 토큰화(tokenization)\n",
    "spacy_de = spacy.load('de_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "인덱스 0: I\n",
      "인덱스 1: am\n",
      "인덱스 2: a\n",
      "인덱스 3: graduate\n",
      "인덱스 4: student\n",
      "인덱스 5: .\n"
     ]
    }
   ],
   "source": [
    "# 간단히 토큰화(tokenization) 기능 써보기\n",
    "tokenized = spacy_en.tokenizer(\"I am a graduate student.\")\n",
    "\n",
    "for i, token in enumerate(tokenized):\n",
    "    print(f\"인덱스 {i}: {token.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 독일어(Deutsch) 문장을 토큰화 하는 함수 (순서를 뒤집지 않음)\n",
    "def tokenize_de(text):\n",
    "    return [token.text for token in spacy_de.tokenizer(text)]\n",
    "\n",
    "# 영어(English) 문장을 토큰화 하는 함수\n",
    "def tokenize_en(text):\n",
    "    return [token.text for token in spacy_en.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import Field, BucketIterator\n",
    "\n",
    "SRC = Field(tokenize=tokenize_de, init_token=\"<sos>\", eos_token=\"<eos>\", lower=True)\n",
    "TRG = Field(tokenize=tokenize_en, init_token=\"<sos>\", eos_token=\"<eos>\", lower=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets import Multi30k\n",
    "\n",
    "train_dataset, valid_dataset, test_dataset = Multi30k.splits(exts=(\".de\", \".en\"), fields=(SRC, TRG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SRC 단어 사전의 총 개수: len(SRC): 7853\n",
      "TRG 단어 사전의 총 개수: len(TRG): 5893\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Field 객체의 build_vocab 메소드를 사용하여 영어와 독어의 단어 사전을 생성한다.\n",
    "min_freq를 사용하여 최소 등장 단어의 개수를 정의한다.\"\"\"\n",
    "SRC.build_vocab(train_dataset, min_freq=2)\n",
    "TRG.build_vocab(train_dataset, min_freq=2)\n",
    "\n",
    "print(f\"SRC 단어 사전의 총 개수: len(SRC): {len(SRC.vocab)}\")\n",
    "print(f\"TRG 단어 사전의 총 개수: len(TRG): {len(TRG.vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4112\n",
      "1752\n"
     ]
    }
   ],
   "source": [
    "print(TRG.vocab.stoi[\"abcabc\"]) # 없는 단어: 0\n",
    "print(TRG.vocab.stoi[TRG.pad_token]) # 패딩(padding): 1\n",
    "print(TRG.vocab.stoi[\"<sos>\"]) # <sos>: 2\n",
    "print(TRG.vocab.stoi[\"<eos>\"]) # <eos>: 3\n",
    "print(TRG.vocab.stoi[\"hello\"])\n",
    "print(TRG.vocab.stoi[\"world\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# 일반적인 데이터 로더(data loader)의 iterator와 유사하게 사용 가능\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_dataset, valid_dataset, test_dataset),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 아키텍처"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 인코더 아키텍처\n",
    "- 주어진 소스 문장을 문맥 벡터(context vector)로 인코딩한다.\n",
    "- 하이퍼 파라미터\n",
    "    - input_dim: 하나의 다넝에 대한 원핫인코딩 차원\n",
    "    - embed_dim: 임베딩 차원\n",
    "    - enc_hidden_dim: 인코더의 hidden_state 차원\n",
    "    - dec_hidden_dim: 디코더의 hidden_state 차원\n",
    "    - dropout_ratio: 드롭아웃 비율"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# 인코더(Encoder) 아키텍처 정의\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim, enc_hidden_dim, dec_hidden_dim, dropout_ratio):\n",
    "        super().__init__()\n",
    "\n",
    "        # 임베딩(embedding)은 원-핫 인코딩(one-hot encoding)을 특정 차원의 임베딩으로 매핑하는 레이어\n",
    "        self.embedding = nn.Embedding(input_dim, embed_dim)\n",
    "\n",
    "        # 양방향(bidirectional) GRU 레이어\n",
    "        self.rnn = nn.GRU(embed_dim, enc_hidden_dim, bidirectional=True)\n",
    "\n",
    "        # FC 레이어\n",
    "        self.fc = nn.Linear(enc_hidden_dim * 2, dec_hidden_dim)\n",
    "\n",
    "        # 드롭아웃(dropout)\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "\n",
    "    # 인코더는 소스 문장을 입력으로 받아 문맥 벡터(context vector)를 반환        \n",
    "    def forward(self, src):\n",
    "        # src: [단어 개수, 배치 크기]: 각 단어의 인덱스(index) 정보\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        # embedded: [단어 개수, 배치 크기, 임베딩 차원]\n",
    "\n",
    "        outputs, hidden = self.rnn(embedded)\n",
    "        # outputs: [단어 개수, 배치 크기, 인코더 히든 차원 * 방향의 수]: 전체 단어의 출력 정보\n",
    "        # hidden: [레이어 개수 * 방향의 수, 배치 크기, 인코더 히든 차원]: 현재까지의 모든 단어의 정보\n",
    "\n",
    "        # hidden은 [forward_1, backward_1, forward_2, backward_2, ...] 형태로 구성\n",
    "        # 따라서 hidden[-2, :, :]은 forwards의 마지막 값\n",
    "        # 따라서 hidden[-1, :, :]은 backwards의 마지막 값\n",
    "        # 디코더(decoder)의 첫 번째 hidden (context) vector는 인코더의 마지막 hidden을 이용\n",
    "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)))\n",
    "\n",
    "        # outputs은 Attention 목적으로, hidden은 context vector 목적으로 사용\n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 디코더 아키텍처"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디코더(Decoder) 아키텍처 정의\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, embed_dim, enc_hidden_dim, dec_hidden_dim, dropout_ratio, attention):\n",
    "        super().__init__()\n",
    "\n",
    "        self.output_dim = output_dim\n",
    "        self.attention = attention\n",
    "\n",
    "        # 임베딩(embedding)은 원-핫 인코딩(one-hot encoding) 말고 특정 차원의 임베딩으로 매핑하는 레이어\n",
    "        self.embedding = nn.Embedding(output_dim, embed_dim)\n",
    "\n",
    "        # GRU 레이어\n",
    "        self.rnn = nn.GRU((enc_hidden_dim * 2) + embed_dim, dec_hidden_dim)\n",
    "\n",
    "        # FC 레이어\n",
    "        self.fc_out = nn.Linear((enc_hidden_dim * 2) + dec_hidden_dim + embed_dim, output_dim)\n",
    "        \n",
    "        # 드롭아웃(dropout)\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "\n",
    "    # 디코더는 현재까지 출력된 문장에 대한 정보를 입력으로 받아 타겟 문장을 반환     \n",
    "    def forward(self, input, hidden, enc_outputs):\n",
    "        # input: [배치 크기]: 단어의 개수는 항상 1개이도록 구현\n",
    "        # hidden: [배치 크기, 히든 차원]\n",
    "        # enc_outputs: [단어 개수, 배치 크기, 인코더 히든 차원 * 방향의 수]: 전체 단어의 출력 정보\n",
    "        input = input.unsqueeze(0)\n",
    "        # input: [단어 개수 = 1, 배치 크기]\n",
    "\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        # embedded: [단어 개수 = 1, 배치 크기, 임베딩 차원]\n",
    "\n",
    "        attention = self.attention(hidden, enc_outputs)\n",
    "        # attention: [배치 크기, 단어 개수]: 실제 각 단어에 대한 어텐선(attention) 값들\n",
    "        attention = attention.unsqueeze(1)\n",
    "        # attention: [배치 크기, 1, 단어 개수]: 실제 각 단어에 대한 어텐선(attention) 값들\n",
    "\n",
    "        enc_outputs = enc_outputs.permute(1, 0, 2)\n",
    "        # enc_outputs: [배치 크기, 단어 개수, 인코더 히든 차원 * 방향의 수]: 전체 단어의 출력 정보\n",
    "\n",
    "        weighted = torch.bmm(attention, enc_outputs) # 행렬 곱 함수\n",
    "        # weighted: [배치 크기, 1, 인코더 히든 차원 * 방향의 수]\n",
    "\n",
    "        weighted = weighted.permute(1, 0, 2)\n",
    "        # weighted: [1, 배치 크기, 인코더 히든 차원 * 방향의 수]\n",
    "        \n",
    "        rnn_input = torch.cat((embedded, weighted), dim=2)\n",
    "        # rnn_input: [1, 배치 크기, 인코더 히든 차원 * 방향의 수 + embed_dim]: 어텐션이 적용된 현재 단어 입력 정보\n",
    "        \n",
    "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
    "        # output: [단어 개수, 배치 크기, 디코더 히든 차원 * 방향의 수]\n",
    "        # hidden: [레이어 개수 * 방향의 수, 배치 크기, 디코더 히든 차원]: 현재까지의 모든 단어의 정보\n",
    "\n",
    "        # 현재 예제에서는 단어 개수, 레이어 개수, 방향의 수 모두 1의 값을 가짐\n",
    "        # 따라서 output: [1, 배치 크기, 디코더 히든 차원], hidden: [1, 배치 크기, 디코더 히든 차원]\n",
    "        # 다시 말해 output과 hidden의 값 또한 동일\n",
    "        assert (output == hidden).all()\n",
    "\n",
    "        embedded = embedded.squeeze(0)\n",
    "        output = output.squeeze(0)\n",
    "        weighted = weighted.squeeze(0)\n",
    "        \n",
    "        prediction = self.fc_out(torch.cat((output, weighted, embedded), dim=1))\n",
    "        # prediction = [배치 크기, 출력 차원]\n",
    "        \n",
    "        # (현재 출력 단어, 현재까지의 모든 단어의 정보)\n",
    "        return prediction, hidden.squeeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 어텐션 아키텍처\n",
    "- 하나의 어텐션은 인코더의 전체 토큰에 대한 출력을 입력으로 받는 FC의 파라미터를 공유하여 사용한다.\n",
    "- (전체 인코더 출력+현재 디코더의 히든) -> 디코더의 히든 = 실제 Attention 값\n",
    "- 하이퍼 파라미터\n",
    "    - enc_hidden_dim: 인코더의 hidden_state 차원\n",
    "    - dec_hidden_dim: 디코더의 hidden_state 차원"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# 어텐션(Attention) 아키텍처 정의\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, enc_hidden_dim, dec_hidden_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attn = nn.Linear((enc_hidden_dim * 2) + dec_hidden_dim, dec_hidden_dim)\n",
    "        self.v = nn.Linear(dec_hidden_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, hidden, enc_outputs):\n",
    "        # hidden: [배치 크기, 히든 차원]: 현재까지의 모든 단어의 정보\n",
    "        # enc_outputs: [단어 개수, 배치 크기, 인코더 히든 차원 * 방향의 수]: 전체 단어의 출력 정보\n",
    "        batch_size = enc_outputs.shape[1]\n",
    "        src_len = enc_outputs.shape[0]\n",
    "\n",
    "        # 현재 디코더의 히든 상태(hidden state)를 src_len만큼 반복\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "        enc_outputs = enc_outputs.permute(1, 0, 2)\n",
    "        # hidden: [배치 크기, 단어 개수, 디코더 히든 차원]: 현재까지의 모든 단어의 정보\n",
    "        # enc_outputs: [배치 크기, 단어 개수, 인코더 히든 차원 * 방향의 수]: 전체 단어의 출력 정보\n",
    "\n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, enc_outputs), dim=2)))\n",
    "        # energy: [배치 크기, 단어 개수, 디코더 히든 차원]\n",
    "\n",
    "        attention = self.v(energy).squeeze(2)\n",
    "        # attention: [배치 크기, 단어 개수]: 실제 각 단어에 대한 어텐선(attention) 값들\n",
    "\n",
    "        return F.softmax(attention, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Seq2Seq with Attention 아키텍처**\n",
    "\n",
    "* 앞서 정의한 인코더(encoder)와 디코더(decoder)를 가지고 있는 하나의 아키텍처입니다.\n",
    "    * **인코더(encoder)**: 주어진 소스 문장을 문맥 벡터(context vector)로 인코딩합니다.\n",
    "    * **디코더(decoder)**: 주어진 문맥 벡터(context vector)를 타겟 문장으로 디코딩합니다.\n",
    "    * 단, **디코더는 한 단어씩** 넣어서 한 번씩 결과를 구합니다.\n",
    "    * 또한 디코더는 문맥 벡터 뿐만 아니라 <b>인코더의 모든 출력을 참고하여 어텐션(attention)을 진행</b>합니다.\n",
    "* **Teacher forcing**: 디코더의 예측(prediction)을 다음 입력으로 사용하지 않고, 실제 목표 출력(ground-truth)을 다음 입력으로 사용하는 기법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    # 학습할 때는 완전한 형태의 소스 문장, 타겟 문장, teacher_forcing_ratio를 넣기\n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        # src: [단어 개수, 배치 크기]\n",
    "        # trg: [단어 개수, 배치 크기]\n",
    "        # 먼저 인코더를 거쳐 전체 출력과 문맥 벡터(context vector)를 추출\n",
    "        enc_outputs, hidden = self.encoder(src)\n",
    "\n",
    "        # 디코더(decoder)의 최종 결과를 담을 텐서 객체 만들기\n",
    "        trg_len = trg.shape[0] # 단어 개수\n",
    "        batch_size = trg.shape[1] # 배치 크기\n",
    "        trg_vocab_size = self.decoder.output_dim # 출력 차원\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "\n",
    "        # 첫 번째 입력은 항상 <sos> 토큰\n",
    "        input = trg[0, :]\n",
    "\n",
    "        # 타겟 단어의 개수만큼 반복하여 디코더에 포워딩(forwarding)\n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden = self.decoder(input, hidden, enc_outputs)\n",
    "\n",
    "            outputs[t] = output # FC를 거쳐서 나온 현재의 출력 단어 정보\n",
    "            top1 = output.argmax(1) # 가장 확률이 높은 단어의 인덱스 추출\n",
    "\n",
    "            # teacher_forcing_ratio: 학습할 때 실제 목표 출력(ground-truth)을 사용하는 비율\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            input = trg[t] if teacher_force else top1 # 현재의 출력 결과를 다음 입력에서 넣기\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **학습(Training)**\n",
    "\n",
    "* 하이퍼 파라미터 설정 및 모델 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TRG.vocab)\n",
    "ENCODER_EMBED_DIM = 256\n",
    "DECODER_EMBED_DIM = 256\n",
    "ENCODER_HIDDEN_DIM = 512\n",
    "DECODER_HIDDEN_DIM = 512\n",
    "ENC_DROPOUT_RATIO = 0.5\n",
    "DEC_DROPOUT_RATIO = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 어텐션(attention) 객체 선언\n",
    "attn = Attention(ENCODER_HIDDEN_DIM, DECODER_HIDDEN_DIM)\n",
    "\n",
    "# 인코더(encoder)와 디코더(decoder) 객체 선언\n",
    "enc = Encoder(INPUT_DIM, ENCODER_EMBED_DIM, ENCODER_HIDDEN_DIM, DECODER_HIDDEN_DIM, ENC_DROPOUT_RATIO)\n",
    "dec = Decoder(OUTPUT_DIM, DECODER_EMBED_DIM, ENCODER_HIDDEN_DIM, DECODER_HIDDEN_DIM, DEC_DROPOUT_RATIO, attn)\n",
    "\n",
    "# Seq2Seq 객체 선언\n",
    "model = Seq2Seq(enc, dec, device).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **모델 가중치 파라미터 초기화**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(7853, 256)\n",
       "    (rnn): GRU(256, 512, bidirectional=True)\n",
       "    (fc): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (attention): Attention(\n",
       "      (attn): Linear(in_features=1536, out_features=512, bias=True)\n",
       "      (v): Linear(in_features=512, out_features=1, bias=False)\n",
       "    )\n",
       "    (embedding): Embedding(5893, 256)\n",
       "    (rnn): GRU(1280, 512)\n",
       "    (fc_out): Linear(in_features=1792, out_features=5893, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)\n",
    "            \n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 학습 및 평가 함수 정의\n",
    "    * 기본적인 Seq2Seq 모델과 완전히 동일하게 작성할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Adam optimizer로 학습 최적화\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# 뒷 부분의 패딩(padding)에 대해서는 값 무시\n",
    "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=TRG_PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습(train) 함수\n",
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    model.train() # 학습 모드\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    # 전체 학습 데이터를 확인하며\n",
    "    for i, batch in enumerate(iterator):\n",
    "        src = batch.src\n",
    "        trg = batch.trg\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(src, trg)\n",
    "        # output: [출력 단어 개수, 배치 크기, 출력 차원]\n",
    "        output_dim = output.shape[-1]\n",
    "        \n",
    "        # 출력 단어의 인덱스 0은 사용하지 않음\n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        # output = [(출력 단어의 개수 - 1) * batch size, output dim]\n",
    "        trg = trg[1:].view(-1)\n",
    "        # trg = [(타겟 단어의 개수 - 1) * batch size]\n",
    "        \n",
    "        # 모델의 출력 결과와 타겟 문장을 비교하여 손실 계산\n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward() # 기울기(gradient) 계산\n",
    "        \n",
    "        # 기울기(gradient) clipping 진행\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        # 파라미터 업데이트\n",
    "        optimizer.step()\n",
    "        \n",
    "        # 전체 손실 값 계산\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 평가(evaluate) 함수\n",
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval() # 평가 모드\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # 전체 평가 데이터를 확인하며\n",
    "        for i, batch in enumerate(iterator):\n",
    "            src = batch.src\n",
    "            trg = batch.trg\n",
    "\n",
    "            # 평가할 때 teacher forcing는 사용하지 않음\n",
    "            output = model(src, trg, 0)\n",
    "            # output: [출력 단어 개수, 배치 크기, 출력 차원]\n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            # 출력 단어의 인덱스 0은 사용하지 않음\n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            # output = [(출력 단어의 개수 - 1) * batch size, output dim]\n",
    "            trg = trg[1:].view(-1)\n",
    "            # trg = [(타겟 단어의 개수 - 1) * batch size]\n",
    "\n",
    "            # 모델의 출력 결과와 타겟 문장을 비교하여 손실 계산\n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            # 전체 손실 값 계산\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 학습(training) 및 검증(validation) 진행\n",
    "    * **학습 횟수(epoch)**: 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 1m 9s\n",
      "\tTrain Loss: 5.035 | Train PPL: 153.691\n",
      "\tValidation Loss: 4.840 | Validation PPL: 126.508\n",
      "Epoch: 02 | Time: 1m 10s\n",
      "\tTrain Loss: 4.167 | Train PPL: 64.547\n",
      "\tValidation Loss: 4.504 | Validation PPL: 90.376\n",
      "Epoch: 03 | Time: 1m 6s\n",
      "\tTrain Loss: 3.524 | Train PPL: 33.911\n",
      "\tValidation Loss: 3.798 | Validation PPL: 44.617\n",
      "Epoch: 04 | Time: 1m 7s\n",
      "\tTrain Loss: 2.953 | Train PPL: 19.157\n",
      "\tValidation Loss: 3.402 | Validation PPL: 30.021\n",
      "Epoch: 05 | Time: 1m 8s\n",
      "\tTrain Loss: 2.536 | Train PPL: 12.626\n",
      "\tValidation Loss: 3.224 | Validation PPL: 25.125\n",
      "Epoch: 06 | Time: 1m 8s\n",
      "\tTrain Loss: 2.233 | Train PPL: 9.324\n",
      "\tValidation Loss: 3.141 | Validation PPL: 23.125\n",
      "Epoch: 07 | Time: 1m 7s\n",
      "\tTrain Loss: 1.965 | Train PPL: 7.137\n",
      "\tValidation Loss: 3.197 | Validation PPL: 24.451\n",
      "Epoch: 08 | Time: 1m 7s\n",
      "\tTrain Loss: 1.781 | Train PPL: 5.938\n",
      "\tValidation Loss: 3.185 | Validation PPL: 24.175\n",
      "Epoch: 09 | Time: 1m 7s\n",
      "\tTrain Loss: 1.606 | Train PPL: 4.982\n",
      "\tValidation Loss: 3.228 | Validation PPL: 25.235\n",
      "Epoch: 10 | Time: 1m 7s\n",
      "\tTrain Loss: 1.470 | Train PPL: 4.349\n",
      "\tValidation Loss: 3.278 | Validation PPL: 26.531\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import math\n",
    "import random\n",
    "\n",
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    start_time = time.time() # 시작 시간 기록\n",
    "\n",
    "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "\n",
    "    end_time = time.time() # 종료 시간 기록\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'seq2seq_with_attention.pt')\n",
    "\n",
    "    print(f'Epoch: {epoch + 1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):.3f}')\n",
    "    print(f'\\tValidation Loss: {valid_loss:.3f} | Validation PPL: {math.exp(valid_loss):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 학습된 모델 저장\n",
    "# from google.colab import files\n",
    "\n",
    "# files.download('seq2seq_with_attention.pt')\n",
    "torch.save(model, './' + 'seq2seq_with_attention.pt') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **모델 최종 테스트(testing) 결과 확인**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 3.266 | Test PPL: 26.215\n"
     ]
    }
   ],
   "source": [
    "model = Seq2Seq(enc, dec, device).to(device)\n",
    "model = torch.load('seq2seq_with_attention.pt')\n",
    "\n",
    "test_loss = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **나만의 데이터로 모델 사용해보기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 번역(translation) 함수\n",
    "def translate_sentence(sentence, src_field, trg_field, model, device, max_len=50):\n",
    "    model.eval() # 평가 모드\n",
    "\n",
    "    if isinstance(sentence, str):\n",
    "        nlp = spacy.load('de')\n",
    "        tokens = [token.text.lower() for token in nlp(sentence)]\n",
    "    else:\n",
    "        tokens = [token.lower() for token in sentence]\n",
    "\n",
    "    # 처음에 <sos> 토큰, 마지막에 <eos> 토큰 붙이기\n",
    "    tokens = [src_field.init_token] + tokens + [src_field.eos_token]\n",
    "    print(f\"전체 소스 토큰: {tokens}\")\n",
    "\n",
    "    src_indexes = [src_field.vocab.stoi[token] for token in tokens]\n",
    "    print(f\"소스 문장 인덱스: {src_indexes}\")\n",
    "\n",
    "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(1).to(device)\n",
    "\n",
    "    # 인코더(endocer)에 소스 문장을 넣어 문맥 벡터(context vector) 계산\n",
    "    with torch.no_grad():\n",
    "        enc_outputs, hidden = model.encoder(src_tensor)\n",
    "\n",
    "    # 처음에는 <sos> 토큰 하나만 가지고 있도록 하기\n",
    "    trg_indexes = [trg_field.vocab.stoi[trg_field.init_token]]\n",
    "\n",
    "    for i in range(max_len):\n",
    "        # 이전에 출력한 단어가 현재 단어로 입력될 수 있도록\n",
    "        trg_tensor = torch.LongTensor([trg_indexes[-1]]).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output, hidden = model.decoder(trg_tensor, hidden, enc_outputs)\n",
    "\n",
    "        pred_token = output.argmax(1).item()\n",
    "        trg_indexes.append(pred_token) # 출력 문장에 더하기\n",
    "\n",
    "        # <eos>를 만나는 순간 끝\n",
    "        if pred_token == trg_field.vocab.stoi[trg_field.eos_token]:\n",
    "            break\n",
    "\n",
    "    # 각 출력 단어 인덱스를 실제 단어로 변환\n",
    "    trg_tokens = [trg_field.vocab.itos[i] for i in trg_indexes]\n",
    "\n",
    "    # 첫 번째 <sos>는 제외하고 출력 문장 반환\n",
    "    return trg_tokens[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "소스 문장: ['eine', 'mutter', 'und', 'ihr', 'kleiner', 'sohn', 'genießen', 'einen', 'schönen', 'tag', 'im', 'freien', '.']\n",
      "타겟 문장: ['a', 'mother', 'and', 'her', 'young', 'song', 'enjoying', 'a', 'beautiful', 'day', 'outside', '.']\n",
      "전체 소스 토큰: ['<sos>', 'eine', 'mutter', 'und', 'ihr', 'kleiner', 'sohn', 'genießen', 'einen', 'schönen', 'tag', 'im', 'freien', '.', '<eos>']\n",
      "소스 문장 인덱스: [2, 8, 364, 10, 134, 70, 624, 565, 19, 780, 200, 20, 88, 4, 3]\n",
      "모델 출력 결과: a mom and her son enjoying enjoying a beautiful day . <eos>\n"
     ]
    }
   ],
   "source": [
    "example_idx = 10\n",
    "\n",
    "src = vars(test_dataset.examples[example_idx])['src']\n",
    "trg = vars(test_dataset.examples[example_idx])['trg']\n",
    "\n",
    "print(f'소스 문장: {src}')\n",
    "print(f'타겟 문장: {trg}')\n",
    "print(\"모델 출력 결과:\", \" \".join(translate_sentence(src, SRC, TRG, model, device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "소스 문장: ['Guten', 'Abend', '.']\n",
      "전체 소스 토큰: ['<sos>', 'guten', 'abend', '.', '<eos>']\n",
      "소스 문장 인덱스: [2, 3798, 1163, 4, 3]\n",
      "모델 출력 결과: chinese workers are lanterns at night . <eos>\n"
     ]
    }
   ],
   "source": [
    "src = tokenize_de(\"Guten Abend.\")\n",
    "\n",
    "print(f'소스 문장: {src}')\n",
    "print(\"모델 출력 결과:\", \" \".join(translate_sentence(src, SRC, TRG, model, device)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 코드 뜯어보기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### batch 데이터 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "첫 번째 배치 크기: torch.Size([128, 28])\n",
      "\n",
      "첫 번째 문장은 모두 <sos> 토큰으로 이루어져 있다\n",
      "tensor([[ 2, 76,  0,  ...,  1,  1,  1],\n",
      "        [ 2,  5, 13,  ...,  1,  1,  1],\n",
      "        [ 2, 43, 41,  ...,  1,  1,  1],\n",
      "        ...,\n",
      "        [ 2,  5, 13,  ...,  1,  1,  1],\n",
      "        [ 2,  5, 70,  ...,  1,  1,  1],\n",
      "        [ 2,  8, 36,  ...,  1,  1,  1]], device='cuda:0')\n",
      "\n",
      "인덱스 0: 2\n",
      "인덱스 1: 2\n",
      "인덱스 2: 2\n",
      "인덱스 3: 2\n",
      "인덱스 4: 2\n",
      "인덱스 5: 2\n",
      "인덱스 6: 2\n",
      "인덱스 7: 2\n",
      "인덱스 8: 2\n",
      "인덱스 9: 2\n",
      "인덱스 10: 2\n",
      "인덱스 11: 2\n",
      "인덱스 12: 2\n",
      "인덱스 13: 2\n",
      "인덱스 14: 2\n",
      "인덱스 15: 2\n",
      "인덱스 16: 2\n",
      "인덱스 17: 2\n",
      "인덱스 18: 2\n",
      "인덱스 19: 2\n",
      "인덱스 20: 2\n",
      "인덱스 21: 2\n",
      "인덱스 22: 2\n",
      "인덱스 23: 2\n",
      "인덱스 24: 2\n",
      "인덱스 25: 2\n",
      "인덱스 26: 2\n",
      "인덱스 27: 2\n",
      "인덱스 28: 2\n",
      "인덱스 29: 2\n",
      "인덱스 30: 2\n",
      "인덱스 31: 2\n",
      "인덱스 32: 2\n",
      "인덱스 33: 2\n",
      "인덱스 34: 2\n",
      "인덱스 35: 2\n",
      "인덱스 36: 2\n",
      "인덱스 37: 2\n",
      "인덱스 38: 2\n",
      "인덱스 39: 2\n",
      "인덱스 40: 2\n",
      "인덱스 41: 2\n",
      "인덱스 42: 2\n",
      "인덱스 43: 2\n",
      "인덱스 44: 2\n",
      "인덱스 45: 2\n",
      "인덱스 46: 2\n",
      "인덱스 47: 2\n",
      "인덱스 48: 2\n",
      "인덱스 49: 2\n",
      "인덱스 50: 2\n",
      "인덱스 51: 2\n",
      "인덱스 52: 2\n",
      "인덱스 53: 2\n",
      "인덱스 54: 2\n",
      "인덱스 55: 2\n",
      "인덱스 56: 2\n",
      "인덱스 57: 2\n",
      "인덱스 58: 2\n",
      "인덱스 59: 2\n",
      "인덱스 60: 2\n",
      "인덱스 61: 2\n",
      "인덱스 62: 2\n",
      "인덱스 63: 2\n",
      "인덱스 64: 2\n",
      "인덱스 65: 2\n",
      "인덱스 66: 2\n",
      "인덱스 67: 2\n",
      "인덱스 68: 2\n",
      "인덱스 69: 2\n",
      "인덱스 70: 2\n",
      "인덱스 71: 2\n",
      "인덱스 72: 2\n",
      "인덱스 73: 2\n",
      "인덱스 74: 2\n",
      "인덱스 75: 2\n",
      "인덱스 76: 2\n",
      "인덱스 77: 2\n",
      "인덱스 78: 2\n",
      "인덱스 79: 2\n",
      "인덱스 80: 2\n",
      "인덱스 81: 2\n",
      "인덱스 82: 2\n",
      "인덱스 83: 2\n",
      "인덱스 84: 2\n",
      "인덱스 85: 2\n",
      "인덱스 86: 2\n",
      "인덱스 87: 2\n",
      "인덱스 88: 2\n",
      "인덱스 89: 2\n",
      "인덱스 90: 2\n",
      "인덱스 91: 2\n",
      "인덱스 92: 2\n",
      "인덱스 93: 2\n",
      "인덱스 94: 2\n",
      "인덱스 95: 2\n",
      "인덱스 96: 2\n",
      "인덱스 97: 2\n",
      "인덱스 98: 2\n",
      "인덱스 99: 2\n",
      "인덱스 100: 2\n",
      "인덱스 101: 2\n",
      "인덱스 102: 2\n",
      "인덱스 103: 2\n",
      "인덱스 104: 2\n",
      "인덱스 105: 2\n",
      "인덱스 106: 2\n",
      "인덱스 107: 2\n",
      "인덱스 108: 2\n",
      "인덱스 109: 2\n",
      "인덱스 110: 2\n",
      "인덱스 111: 2\n",
      "인덱스 112: 2\n",
      "인덱스 113: 2\n",
      "인덱스 114: 2\n",
      "인덱스 115: 2\n",
      "인덱스 116: 2\n",
      "인덱스 117: 2\n",
      "인덱스 118: 2\n",
      "인덱스 119: 2\n",
      "인덱스 120: 2\n",
      "인덱스 121: 2\n",
      "인덱스 122: 2\n",
      "인덱스 123: 2\n",
      "인덱스 124: 2\n",
      "인덱스 125: 2\n",
      "인덱스 126: 2\n",
      "인덱스 127: 2\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(train_iterator):\n",
    "    first_batch_src = batch.src\n",
    "    first_batch_trg = batch.trg\n",
    "\n",
    "    print(f\"첫 번째 배치 크기: {first_batch_src.shape}\")\n",
    "    print()\n",
    "    print('첫 번째 문장은 모두 <sos> 토큰으로 이루어져 있다')\n",
    "    print(first_batch_src)\n",
    "    print()\n",
    "\n",
    "    # 현재 배치에 있는 하나의 문장에 포함된 정보 출력\n",
    "    for i in range(first_batch_src.shape[0]):\n",
    "        print(f\"인덱스 {i}: {first_batch_src[i][0].item()}\")\n",
    "\n",
    "    # 첫 번째 배치만 확인\n",
    "        # 여기서 첫 번째 배치란 1열 모두를 의미한다. tensor의 shape이 (30, 128)로 되어있기 때문\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 인코더 아키텍처 test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "첫 번째 배치 크기: torch.Size([128, 37])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"첫 번째 batch만 가져와서 확인하기\"\"\"\n",
    "## 첫 번째 배치만 확인\n",
    "    # 여기서 첫 번째 배치란 1열 모두를 의미한다. tensor의 shape이 (30, 128)로 되어있기 때문\n",
    "for i, batch in enumerate(train_iterator):\n",
    "    first_batch_src = batch.src\n",
    "    first_batch_trg = batch.trg\n",
    "    print(f\"첫 번째 배치 크기: {first_batch_src.shape}\")\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw 입력값 : torch.Size([128, 37])\n",
      "첫 번째 batch의 한 문장 : tensor([  2,   8,  16, 184,  21,   6,  46, 107,  17,  81,  14,  94,  16,   4,\n",
      "          3,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "\"\"\"인코더(encoder) 객체 선언\"\"\"\n",
    "INPUT_DIM = len(SRC.vocab) # 7853로 전체 단어 사전의 길이를 의미\n",
    "OUTPUT_DIM = len(TRG.vocab) # 5893로 전체 번역해야 하는 단어 사전의 길이를 의미\n",
    "ENCODER_EMBED_DIM = 256 # 인코더의 임베딩 차원\n",
    "DECODER_EMBED_DIM = 256 # 디코더의 임베딩 차원\n",
    "ENCODER_HIDDEN_DIM = 512 # 인코더 GRU의 hidden 차원\n",
    "DECODER_HIDDEN_DIM = 512 # 디코더 GRU의 hidden 차원\n",
    "ENC_DROPOUT_RATIO = 0.5 # 인코더 dropout 비율\n",
    "DEC_DROPOUT_RATIO = 0.5 # 디코더 dropout 비율\n",
    "\n",
    "## 인코더 입력에 대입\n",
    "# enc = Encoder(INPUT_DIM=7853, ENCODER_EMBED_DIM=256, ENCODER_HIDDEN_DIM=512, DECODER_HIDDEN_DIM=512, ENC_DROPOUT_RATIO=0.5)\n",
    "# enc = Encdoer(input_dim=7853, embed_dim=256,         enc_hidden_dim=512,     dec_hidden_dim=512,     dropout_ratio=0.5    )\n",
    "\n",
    "## 첫 번째 batch에 대한 입력값 설정\n",
    "input_src = first_batch_src.clone()\n",
    "input_trg = first_batch_trg.clone()\n",
    "print('raw 입력값 :', input_src.shape)\n",
    "\n",
    "# 첫 번째 batch의 한 문장을 의미\n",
    "print('첫 번째 batch의 한 문장 :',input_src[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding layer : Embedding(7853, 256)\n",
      "첫 batch의 임베딩 값 차원 : torch.Size([128, 37, 256])\n",
      "\n",
      "한 문장은 28개로 이루어져있고, 임베딩을 거치면 각 문장의 단어는 256차원으로 분해된다. \n",
      " 따라서 아래와 같이 (1,28)이 (28, 256)으로 변하는 것이다. \n",
      "\n",
      "  tensor([[-0.7793,  0.4336,  0.3997,  ...,  0.0616, -1.1276,  1.4298],\n",
      "        [-0.4862, -1.1893,  0.3279,  ..., -0.5035,  1.4744,  0.7188],\n",
      "        [-0.5590, -0.0923, -0.4152,  ..., -0.4922,  0.1253,  1.4944],\n",
      "        ...,\n",
      "        [ 0.5661,  0.1912,  0.9236,  ..., -1.5110,  0.2699, -0.7312],\n",
      "        [ 0.5661,  0.1912,  0.9236,  ..., -1.5110,  0.2699, -0.7312],\n",
      "        [ 0.5661,  0.1912,  0.9236,  ..., -1.5110,  0.2699, -0.7312]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "## 1. 임베딩 레이어\n",
    "self_encoder_embedding = nn.Embedding(INPUT_DIM, ENCODER_EMBED_DIM).cuda()\n",
    "print('embedding layer :', self_encoder_embedding)\n",
    "\n",
    "## 임베딩 레이어\n",
    "    # [batch_size, (one batch 내의) 단어 개수, embedding dimension]\n",
    "embedded = self_encoder_embedding(input_src)\n",
    "print('첫 batch의 임베딩 값 차원 :', embedded.shape)\n",
    "print()\n",
    "print('한 문장은 28개로 이루어져있고, 임베딩을 거치면 각 문장의 단어는 256차원으로 분해된다. \\n 따라서 아래와 같이 (1,28)이 (28, 256)으로 변하는 것이다. \\n\\n ', embedded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rnn layer : GRU(256, 512, batch_first=True, bidirectional=True)\n",
      "output : torch.Size([128, 37, 1024])\n",
      "hidden : torch.Size([2, 128, 512])\n"
     ]
    }
   ],
   "source": [
    "## 2. 인코더의 GRU 레이어\n",
    "    # 임베딩된 차원을 입력으로 받아서 hidden 차원을 출력한다.\n",
    "self_rnn = nn.GRU(ENCODER_EMBED_DIM, ENCODER_HIDDEN_DIM, bidirectional=True, batch_first = True).cuda()\n",
    "print('rnn layer :', self_rnn)\n",
    "\n",
    "# outputs: [batch_size, (one batch 내의) 단어 개수, encoder_hidden_dim*방향의 수] : 전체 단어의 출력 정보\n",
    "# hidden: [num_layer * 방향의 수, batch_size, encoder_hidden_dim]: 현재까지의 모든 단어의 정보\n",
    "output_rnn, hidden_rnn = self_rnn(embedded)\n",
    "print('output :', output_rnn.shape)\n",
    "print('hidden :', hidden_rnn.shape) # hidden 차원은 batch가 중간에 오게 됨\n",
    "    # hidden은 [forward_layer1, backward_layer1, forward_layer2(두 번째 forward 레이어), backward_layer2(두 번째 backward 레이어), ...] 형태로 구성\n",
    "    # 따라서 hidden[-2, :, :]은 forwards의 마지막 값\n",
    "    # 따라서 hidden[-1, :, :]은 backwards의 마지막 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FC layer : Linear(in_features=1024, out_features=512, bias=True)\n",
      "\n",
      "전체 rnn의 hidden : torch.Size([2, 128, 512])\n",
      "forward rnn hidden : torch.Size([128, 512])\n",
      "backward rnn hidden : torch.Size([128, 512])\n",
      "\n",
      "FC layer에 입력 값 : torch.Size([128, 1024])\n",
      "인코더의 첫 번째 마지막 값(RNN에서 나온 최종 결과값) : torch.Size([128, 37, 1024])\n",
      "인코더의 두 번째 마지막 값(FC layer에서 나온 최종 결과값) : torch.Size([128, 512])\n"
     ]
    }
   ],
   "source": [
    "## 3. 인코더의 FC 레이어\n",
    "    # 인코더의 최종 hidden은 rnn의 forward hidden의 마지막 값과 backward hidden의 마지막 값을 concat하여 FC layer에 사용\n",
    "    # 디코더(decoder)의 첫 번째 hidden (context) vector는 인코더의 마지막 hidden을 이용\n",
    "self_fc = nn.Linear(ENCODER_HIDDEN_DIM * 2, DECODER_HIDDEN_DIM).cuda()\n",
    "print('FC layer :', self_fc, end='\\n\\n')\n",
    "print('전체 rnn의 hidden :', hidden_rnn.shape) \n",
    "print('forward rnn hidden :', hidden_rnn[-2, :, :].shape) # [batch_size, encoder_hidden_dim]\n",
    "print('backward rnn hidden :', hidden_rnn[-1, :, :].shape, end='\\n\\n') # [batch_size, encoder_hidden_dim]\n",
    "\n",
    "# forward layer의 최종 hidden값과 backward layer의 최종 hidden 값을 concat하여 fc layer에 넣어줌\n",
    "    # 위 결과가 인코더의 최종 값이 된다.\n",
    "encoder_final_hidden = torch.tanh(self_fc(torch.cat((hidden_rnn[-2,:,:], hidden_rnn[-1,:,:]), dim=1)))\n",
    "print('FC layer에 입력 값 :', torch.cat((hidden_rnn[-2,:,:], hidden_rnn[-1,:,:]), dim=1).shape) # [batch_size, encoder_hidden_dim*2]\n",
    "\n",
    "print('인코더의 첫 번째 마지막 값(RNN에서 나온 최종 결과값) :', output_rnn.shape) # [batch_size, 입력 문장 길이, rnn의 hidden feature 차원]\n",
    "print('인코더의 두 번째 마지막 값(FC layer에서 나온 최종 결과값) :', encoder_final_hidden.shape) # [batch_size, decoder_hidden_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'전체 모델 구조'"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"전체 모델 구조\"\"\"\n",
    "# # 어텐션(attention) 객체 선언\n",
    "# attn = Attention(ENCODER_HIDDEN_DIM, DECODER_HIDDEN_DIM)\n",
    "\n",
    "# # 인코더(encoder)와 디코더(decoder) 객체 선언\n",
    "# enc = Encoder(INPUT_DIM, ENCODER_EMBED_DIM, ENCODER_HIDDEN_DIM, DECODER_HIDDEN_DIM, ENC_DROPOUT_RATIO)\n",
    "# dec = Decoder(OUTPUT_DIM, DECODER_EMBED_DIM, ENCODER_HIDDEN_DIM, DECODER_HIDDEN_DIM, DEC_DROPOUT_RATIO, attn)\n",
    "\n",
    "# # Seq2Seq 객체 선언\n",
    "# model = Seq2Seq(enc, dec, device).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 디코더 & 어텐션 아키텍처 test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"디코더에 들어가는 인코더의 최종 결과'"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"디코더의 입력값은 총 3개\"\"\"\n",
    "## 1. <sos> 토큰으로 시작하는 값\n",
    "    # batch별 문장의 <sos> 토큰 값이 들어감\n",
    "## 2~3. 인코더에서 나온 결과 2개\n",
    "    # 인코더 rnn output과 rnn의 hidden state를 fc layer에 넣어서 만든 값\n",
    "\"\"\"\"디코더에 들어가는 인코더의 최종 결과\"\"\"\n",
    "## output_rnn과 encoder_final_hidden\n",
    "    # output_rnn, encoder_final_hidden = self.encoder(input_first_batch_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"디코더(decoder) 객체 선언\"\"\"\n",
    "INPUT_DIM = len(SRC.vocab) # 7853로 전체 단어 사전의 길이를 의미\n",
    "OUTPUT_DIM = len(TRG.vocab) # 5893로 전체 번역해야 하는 단어 사전의 길이를 의미\n",
    "ENCODER_EMBED_DIM = 256 # 인코더의 임베딩 차원\n",
    "DECODER_EMBED_DIM = 256 # 디코더의 임베딩 차원\n",
    "ENCODER_HIDDEN_DIM = 512 # 인코더 GRU의 hidden 차원\n",
    "DECODER_HIDDEN_DIM = 512 # 디코더 GRU의 hidden 차원\n",
    "ENC_DROPOUT_RATIO = 0.5 # 인코더 dropout 비율\n",
    "DEC_DROPOUT_RATIO = 0.5 # 디코더 dropout 비율\n",
    "\n",
    "## 디코더 입력에 대입\n",
    "# enc = Decoder(OUTPUT_DIM=5893, DECODER_EMBED_DIM=256, ENCODER_HIDDEN_DIM=512, DECODER_HIDDEN_DIM=512, DEC_DROPOUT_RATIO=0.5, attn)\n",
    "# enc = Decdoer(output_dim=5893, embed_dim=256,         enc_hidden_dim=512,     dec_hidden_dim=512,     dropout_ratio=0.5,     attention(클래스))\n",
    "\n",
    "## 첫 번째 batch에 대한 입력값 설정\n",
    "input_src = first_batch_src.clone()\n",
    "input_trg = first_batch_trg.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "첫 번째 batch에 대한 번역 문장의 단어 개수 : 36\n",
      "\n",
      "batch size : 128\n",
      "\n",
      "출력 차원 : 5893\n",
      "\n",
      "디코더의 최종 결과를 담을 텐서 객체 : torch.Size([128, 36, 5893])\n",
      "\n",
      "디코더 첫 입력값 <sos> 토큰 : tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
      "torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"1-1. 디코더 입력값 설정\"\"\"\n",
    "## 인코더의 최종 hidden과 인코더의 최종 output을 비교하는 게 attention의 첫 시작\n",
    "    # input: [배치 크기]: 단어의 개수는 항상 1개이도록 구현\n",
    "    # hidden: [배치 크기, 히든 차원] (인코더의 최종 hidden)\n",
    "    # enc_outputs: [단어 개수, 배치 크기, 인코더 히든 차원 * 방향의 수]: 전체 단어의 출력 정보\n",
    "\n",
    "## 디코더(decoder)의 최종 결과를 담을 텐서 객체 만들기\n",
    "    # input_trg : [batch_size, 번역해야 하는 단어의 길이]\n",
    "trg_len = input_trg.shape[1]\n",
    "print('첫 번째 batch에 대한 번역 문장의 단어 개수 :', trg_len, end='\\n\\n')\n",
    "\n",
    "    # 배치 크기\n",
    "batch_size = input_trg.shape[0]\n",
    "print('batch size :', batch_size, end='\\n\\n')\n",
    "\n",
    "    # 출력 차원\n",
    "        # 출력해야하는 trg에 대한 단어 사전 임베딩 차원\n",
    "        # 5893 -> 256\n",
    "trg_vocab_size = OUTPUT_DIM \n",
    "print('출력 차원 :', trg_vocab_size, end='\\n\\n')\n",
    "    \n",
    "    # 디코더의 최종 결과를 담을 텐서 객체\n",
    "        # [batch_size, 한 배치에 대한 문장의 단어 개수, 출력하는 임베딩 차원]\n",
    "outputs_decoder = torch.zeros(batch_size, trg_len, trg_vocab_size)\n",
    "print('디코더의 최종 결과를 담을 텐서 객체 :', outputs_decoder.shape , end='\\n\\n')\n",
    "\n",
    "\n",
    "## batch별 첫 번째 입력은 항상 <sos> 토큰\n",
    "input_decoder = input_trg[:,0]\n",
    "print('디코더 첫 입력값 <sos> 토큰 :', input_decoder)\n",
    "print(input_decoder.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 1])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"1-2. 디코더 입력값 설정\"\"\"\n",
    "## 디코더는 현재까지 출력된 문장에 대한 정보를 입력으로 받아 타겟 문장을 반환\n",
    "## self.decoder(input, hidden, enc_outputs)\n",
    "    # input = [1개 단어] : 단어의 개수는 항상 1개이도록 구현\n",
    "    # hidden = [batch_size, 인코더 hidden feature 차원] : 인코더의 최종 fc layer를 거친 hidden 값\n",
    "    # enc_outputs = [batch_size, 단어 개수, 인코더 hidden 차원*방향의 수] : 인코더 RNN의 출력값\n",
    "input_decoder_ = input_decoder.unsqueeze(1)\n",
    "print(input_decoder_.shape, end='\\n\\n') # [배치 크기, 단어 개수]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "임베딩된 디코더 값: torch.Size([128, 1, 256])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"2. 디코더 임베딩 레이어\"\"\"\n",
    "decoder_self_embedding = nn.Embedding(OUTPUT_DIM, DECODER_EMBED_DIM).to(device) # 디코더에서 번역해야하는 단어 목록을 임베딩\n",
    "decoder_embedded = decoder_self_embedding(input_decoder_)\n",
    "print('임베딩된 디코더 값:', decoder_embedded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재의 디코더 hidden값 : torch.Size([128, 37, 512])\n",
      "현재의 인코더 output 값 : torch.Size([128, 37, 1024])\n",
      "\n",
      "어텐션 에너지 입력값 : torch.Size([128, 37, 1536])\n",
      "에너지 차원 : torch.Size([128, 37, 512])\n",
      "소프트맥스 전의 최종 어텐션 값 : torch.Size([128, 37])\n",
      "소프트맥스 후의 어텐션 값 : torch.Size([128, 37])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"3. 어텐션 아키텍처\"\"\"\n",
    "batch_size = output_rnn.shape[0]\n",
    "src_len = output_rnn.shape[1] # 단어 길이\n",
    "\n",
    "## 현재 디코더의 hidden state를 src_len만큼 반복\n",
    "    # 여기서 현재 디코더의 hidden_state의 '첫번째 값'은 인코더으 최종 hidden state를 의미하고, 이후 forwarding이 진행될 때 업데이트된다.\n",
    "hidden_att = encoder_final_hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "enc_outputs = output_rnn # 위의 코드에서는 batch_first=False이기에 .permute(1,0,2)를 해주어야 한다.\n",
    "print('현재의 디코더 hidden값 :', hidden_att.shape) # hidden_att : [배치 크기, 단어 개수, 디코더 히든 차원]: 현재까지의 모든 단어의 정보\n",
    "print('현재의 인코더 output 값 :', enc_outputs.shape) # enc_outputs : [배치 크기, 단어 개수, 인코더 히든 차원 * 방향의 수]: 전체 단어의 출력 정보\n",
    "print()\n",
    "\n",
    "## 어텐션 에너지 만들기\n",
    "    # 현재의 디코더 hidden 값과 인코더 최종 output 값의 feature concat\n",
    "energy_input = torch.cat((hidden_att, enc_outputs), dim=2)\n",
    "print('어텐션 에너지 입력값 :', energy_input.shape)\n",
    "\n",
    "self_attn = nn.Linear((ENCODER_HIDDEN_DIM * 2) + DECODER_HIDDEN_DIM, DECODER_HIDDEN_DIM).to(device)\n",
    "# print('어텐션 레이어 :', self_attn)\n",
    "\n",
    "energy = self_attn(energy_input) # energy: [배치 크기, 단어 개수, 디코더 히든 차원]\n",
    "print('에너지 차원 :', energy.shape) \n",
    "\n",
    "self_v = nn.Linear(DECODER_HIDDEN_DIM, 1, bias=False).to(device) # attention: [배치 크기, 단어 개수]: 실제 각 단어에 대한 어텐선(attention) 값들\n",
    "attention = self_v(energy).squeeze(2)\n",
    "print('소프트맥스 전의 최종 어텐션 값 :', attention.shape) # 이를 통해 각 문장의 단어 28개에 대한 가중치를 구한다.\n",
    "print('소프트맥스 후의 어텐션 값 :', F.softmax(attention, dim=1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "실제 각 단어에 대한 attention 값들 : torch.Size([128, 1, 37])\n",
      "\n",
      "인코더 rnn 출력값 : torch.Size([128, 37, 1024])\n",
      "\n",
      "weighted shape : torch.Size([128, 1, 1024])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"4. 어텐션 weight 반영\"\"\"\n",
    "attention = attention.unsqueeze(1) #  attention: [배치 크기, 1, 단어 개수]: 실제 각 단어에 대한 어텐선(attention) 값들\n",
    "print('실제 각 단어에 대한 attention 값들 :', attention.shape, end='\\n\\n')\n",
    "\n",
    "enc_outputs = output_rnn # [배치 크기, 단어 개수, 인코더 히든 차원 * 방향의 수]: 전체 단어의 출력 정보\n",
    "print('인코더 rnn 출력값 :', enc_outputs.shape, end='\\n\\n')\n",
    "\n",
    "weighted = torch.bmm(attention, enc_outputs) # 행렬곱 함수\n",
    "# (batch_size, 1, 28) * (batch_size, 28, 1024)\n",
    "print('weighted shape :', weighted.shape) # [배치 크기, 1, 인코더 히든 차원 * 방향의 수]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "디코더 rnn의 입력값 : torch.Size([128, 1, 1280])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"5. 디코더 임베딩과 어텐션 weight 값의 feature들 concat\"\"\"\n",
    "decoder_rnn_input = torch.cat((decoder_embedded, weighted), dim=2) # rnn_input: [1, 배치 크기, 인코더 히든 차원 * 방향의 수 + embed_dim]: 어텐션이 적용된 현재 단어 입력 정보\n",
    "print('디코더 rnn의 입력값 :', decoder_rnn_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "디코더의 rnn 결과 : torch.Size([128, 1, 512])\n",
      "디코더의 hidden 값 결과 : torch.Size([1, 128, 512])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"6. 디코더의 출력값 생성 (rnn 사용)\"\"\"\n",
    "self_rnn = nn.GRU((ENCODER_HIDDEN_DIM * 2) + DECODER_EMBED_DIM, DECODER_HIDDEN_DIM, batch_first=True).to(device)\n",
    "output, hidden = self_rnn(decoder_rnn_input, encoder_final_hidden.unsqueeze(0))\n",
    "print('디코더의 rnn 결과 :', output.shape) # output: [단어 개수, 배치 크기, 디코더 히든 차원 * 방향의 수]\n",
    "print('디코더의 hidden 값 결과 :',hidden.shape) # hidden: [레이어 개수 * 방향의 수, 배치 크기, 디코더 히든 차원]: 현재까지의 모든 단어의 정보\n",
    "\n",
    "## 출력은 단 하나의 값\n",
    "    # 현재 예제에서는 단어 개수, 레이어 개수, 방향의 수 모두 1의 값을 가짐\n",
    "    # 따라서 output: [1, 배치 크기, 디코더 히든 차원], hidden: [1, 배치 크기, 디코더 히든 차원]\n",
    "    # 다시 말해 output과 hidden의 값 또한 동일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 256])\n",
      "torch.Size([128, 512])\n",
      "torch.Size([128, 1024])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"최종 FC layer\"\"\"\n",
    "decoder_embedded_ = decoder_embedded.squeeze(1)\n",
    "output_ = output.squeeze(1) # 디코더 rnn의 결과\n",
    "weighted_ = weighted.squeeze(1) # 어텐션 weight\n",
    "print(decoder_embedded_.shape)\n",
    "print(output_.shape)\n",
    "print(weighted_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최종 FC layer : Linear(in_features=1792, out_features=5893, bias=True)\n",
      "\n",
      "최종 예측값 : torch.Size([128, 5893])\n"
     ]
    }
   ],
   "source": [
    "self_fc_out = nn.Linear((ENCODER_HIDDEN_DIM * 2) + DECODER_HIDDEN_DIM + DECODER_EMBED_DIM, OUTPUT_DIM).to(device)\n",
    "print('최종 FC layer :', self_fc_out, end='\\n\\n')\n",
    "prediction = self_fc_out(torch.cat((output_, weighted_, decoder_embedded_), dim=1))\n",
    "print('최종 예측값 :', prediction.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 타겟 단어의 개수만큼 반복하여 디코더에 포워딩(forwarding)\n",
    "# for t in range(1, trg_len):\n",
    "#     output, hidden = self.decoder(input, hidden, enc_outputs) # 여기서 hidden은 인코더 hidden\n",
    "\n",
    "#     outputs[t] = output # FC를 거쳐서 나온 현재의 출력 단어 정보\n",
    "#     top1 = output.argmax(1) # 가장 확률이 높은 단어의 인덱스 추출\n",
    "\n",
    "#     # teacher_forcing_ratio: 학습할 때 실제 목표 출력(ground-truth)을 사용하는 비율\n",
    "#     teacher_force = random.random() < teacher_forcing_ratio\n",
    "#     input = trg[t] if teacher_force else top1 # 현재의 출력 결과를 다음 입력에서 넣기"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.11 ('torch-gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "69f443095525f2467c034a863fbb849f2621b9ca87839daf8c48ed874bc8a4ae"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
